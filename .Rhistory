quanteda::tokens_remove(quanteda::stopwords("en")) %>%
quanteda::tokens_tolower()
# Get unique words
allWords <- unique(unlist(clText))
return(allWords)
}
filter_corpus(tidyCorp, keepPOS)
# source("sean_functions.R")
source("emmy_functions.R")
runApp()
x <- filter_corpus(tidyCorp, keepPOS)
x
wordList
tagCorpCl <- tidyCorp %>%
filter(tolower(lemma) %in% x)
View(tagCorpCl)
load_glove = function(filename, corpus, n=5000, minTermFrequency = 2) {
#' Filter domain relevant vocabulary from pre-trained GloVe embeddings.
#' @param filename path to pre-trained GloVe word embedding text file.
#' @param corpus cleaned text corpus
#' @param n total top words to keep from cleaned text corpus
#' @param minTermFrequency minimum occurrences of a word in training text
#' @return list of embeddings relevant to domain vocabulary.
print("üîπ Step 1: Starting load_glove() function")
# Load Required Packages
require(vroom, quietly = TRUE)
print("üîπ Step 2: Checking if file exists")
filename = path.expand(filename)
if (!file.exists(filename)) {
stop(paste0("‚ùå Error: File ", filename, " does not exist"))
}
print("‚úÖ File found: Proceeding with loading vocabulary")
print("üîπ Step 3: Extracting vocabulary from corpus")
vocabulary <- tryCatch({
get_vocabulary(corpus, minTermFrequency = minTermFrequency)  %>%
dplyr::arrange(desc(TotCount)) %>%
dplyr::slice_head(n=n) %>%
dplyr::select(word, "total_count" = TotCount, "part_of_speech" = pos, entity_type)
}, error = function(e) {
stop(paste0("‚ùå Error in get_vocabulary(): ", e$message))
})
print(paste0("‚úÖ Extracted vocabulary: ", nrow(vocabulary), " words found"))
print("üîπ Step 4: Cleaning vocabulary")
vocabSplit <- tryCatch({
clean_vocabulary(vocabulary$word)
}, error = function(e) {
stop(paste0("‚ùå Error in clean_vocabulary(): ", e$message))
})
print(paste0("‚úÖ Vocabulary cleaned: ", length(vocabSplit), " words retained"))
gc()
print("üîπ Step 5: Loading GloVe model")
gl_model <- tryCatch({
vroom::vroom(filename, delim = " ", vroom::locale(encoding = "UTF-8"), quote = "Z",
skip_empty_rows = TRUE, skip = 1) %>%
dplyr::filter(X1 %in% vocabSplit)
}, error = function(e) {
stop(paste0("‚ùå Error loading GloVe embeddings: ", e$message))
})
print(paste0("‚úÖ Loaded GloVe model with ", nrow(gl_model), " words matching vocabulary"))
gc()
print("üîπ Step 6: Normalizing GloVe vectors")
if (ncol(gl_model) > 1) {
gl_model <- tryCatch({
gl_model %>%
dplyr::mutate(across(2:ncol(gl_model), ~ scale(., center = FALSE, scale = norm_L2(.))))
}, error = function(e) {
stop(paste0("‚ùå Error in normalization step: ", e$message))
})
print("‚úÖ Normalization complete")
} else {
stop("‚ùå Error: GloVe model has only one column (word names) and no embeddings")
}
print("üîπ Step 7: Renaming columns")
colnames(gl_model) = c('word', paste('dim', 1:(ncol(gl_model)-1), sep = '_'))
print("‚úÖ Column renaming done")
model_results <- list("metadata" = vocabulary, "embeddings" = gl_model)
print("‚úÖ Function successfully completed! Returning results.")
return(model_results)
}
modelPath
glove_data <- load_glove(modelPath, tagCorpCl, n=5000, minTermFrequency=2)
tagCorpCl
get_vocabulary(tagCorpCl)
get_vocabulary <- function(parsedText, minTermFrequency = 2) {
#' Get filteres list of unique corpus domain relevant words.
#' @param parsedText fully cleaned corpus with tagging and filtering.
#' @param minTermFrequency minimum number of times a word appears in a text to be kept.
#' @return unique list of words in corpus.
#### Master entities
parsedText$entity_type[is.na(parsedText$entity_type) | parsedText$entity_type == ""] <- "X"
parsedText$word <- stringr::str_trim(tolower(parsedText$lemma), side = c("both"))
### Parts of Speech
parsedTmp <- parsedText %>%
dplyr::group_by(word, entity_type) %>%
dplyr::summarise(Count = dplyr::n(), pos = upos) %>%
dplyr::distinct(word, entity_type, Count, pos, .keep_all = TRUE) %>%
dplyr::ungroup()
parsedTmp <- parsedTmp %>%
dplyr::group_by(word) %>%
dplyr::mutate(Percentage=round(Count/sum(Count)*100,2), pos = upos) %>%
dplyr::slice_max(order_by = Percentage, n = 1) %>%
dplyr::distinct(word, .keep_all = TRUE)
###
parsedTmp <- parsedTmp %>%
dplyr::group_by(word, entity_type) %>%
dplyr::summarise(TotCount = sum(Count), Count = dplyr::n(), Percentage = Percentage, pos = pos) %>%
dplyr::ungroup()
parsedTmp <- parsedTmp %>%
dplyr::group_by(word) %>%
dplyr::slice_max(order_by = Count, n = 1) %>%
dplyr::distinct(word, .keep_all = TRUE)
parsedTextFin <- parsedTmp %>%
dplyr::select(!(c(Count))) %>%
dplyr::ungroup() %>%
dplyr::filter(TotCount >= minTermFrequency)
return(parsedTextFin)
}
get_vocabulary(tagCorpCl)
get_vocabulary <- function(parsedText, minTermFrequency = 2) {
#' Get filtered list of unique corpus domain relevant words.
#' @param parsedText fully cleaned corpus with tagging and filtering.
#' @param minTermFrequency minimum number of times a word appears in a text to be kept.
#' @return unique list of words in corpus.
print("üîπ Step 1: Checking and cleaning entity_type column")
# Ensure entity_type column exists, otherwise create it
if (!"entity_type" %in% colnames(parsedText)) {
parsedText$entity_type <- "X"
} else {
parsedText$entity_type <- ifelse(is.na(parsedText$entity_type) | parsedText$entity_type == "", "X", parsedText$entity_type)
}
print("‚úÖ entity_type column cleaned")
print("üîπ Step 2: Creating word column from lemma")
parsedText$word <- stringr::str_trim(tolower(parsedText$lemma), side = "both")
print("‚úÖ Word column created")
### Parts of Speech Processing
print("üîπ Step 3: Processing POS tagging")
parsedTmp <- parsedText %>%
dplyr::group_by(word, entity_type) %>%
dplyr::summarise(Count = dplyr::n(), pos = first(upos), .groups = "drop") %>%
dplyr::distinct(word, entity_type, Count, pos, .keep_all = TRUE)
print("‚úÖ POS tagging done")
print("üîπ Step 4: Computing percentages and filtering words")
parsedTmp <- parsedTmp %>%
dplyr::group_by(word) %>%
dplyr::mutate(Percentage = round(Count / sum(Count) * 100, 2)) %>%
dplyr::slice_max(order_by = Percentage, n = 1) %>%
dplyr::distinct(word, .keep_all = TRUE)
print("‚úÖ Percentage calculations complete")
### Summarizing Counts
print("üîπ Step 5: Summarizing word occurrences")
parsedTmp <- parsedTmp %>%
dplyr::group_by(word, entity_type) %>%
dplyr::summarise(TotCount = sum(Count), Percentage = first(Percentage), pos = first(pos), .groups = "drop")
print("‚úÖ Word summarization done")
print("üîπ Step 6: Filtering words based on minTermFrequency")
parsedTextFin <- parsedTmp %>%
dplyr::filter(TotCount >= minTermFrequency) %>%
dplyr::ungroup()
print("‚úÖ Filtering complete")
return(parsedTextFin)
}
get_vocabulary(tagCorpCl)
# source("sean_functions.R")
source("emmy_functions.R")
runApp()
glove_data <- load_glove(modelPath, tagCorpCl, n=5000, minTermFrequency=2)
View(glove_data)
multi_word_avg <- function(vocabulary, vectors) {
print("üîπ Step 1: Extracting word vectors for multi-word terms")
dictionarySplit <- stringr::str_split(vocabulary, pattern = "[- _]")
cleanWords <- clean_vocabulary(vocabulary)
# Check if embeddings exist
if (!"embeddings" %in% names(vectors)) {
stop("‚ùå Error: `vectors` does not contain an `embeddings` element.")
}
wordVec <- vectors$embeddings %>%
dplyr::filter(word %in% tolower(as.character(unique(unlist(dictionarySplit)))))
# Check if wordVec is empty
if (nrow(wordVec) == 0) {
stop("‚ùå Error: No words from `vocabulary` found in `vectors$embeddings`.")
}
cleanVec <- wordVec[FALSE,]
print(paste0("‚úÖ Extracted ", nrow(wordVec), " words for vector averaging."))
for (i in seq_along(dictionarySplit)) {
wordInd <- tolower(unlist(dictionarySplit[[i]]))
wordFull <- paste(wordInd, collapse = "_")
rawVec <- wordVec %>%
dplyr::filter(word %in% wordInd)
if (nrow(rawVec) > 1) {
tmpVec <- rawVec[, -1] %>%
dplyr::summarise(dplyr::across(.cols = everything(), mean)) %>%
dplyr::mutate(word = wordFull) %>%
dplyr::relocate(word)
} else if (nrow(rawVec) == 1) {
tmpVec <- rawVec[, -1] %>%
dplyr::mutate(word = wordFull) %>%
dplyr::relocate(word)
} else {
next  # Skip if no matching words
}
cleanVec <- dplyr::bind_rows(cleanVec, tmpVec)
}
if (nrow(cleanVec) == 0) {
stop("‚ùå Error: `multi_word_avg` produced an empty result.")
}
cleanVec <- dplyr::distinct(cleanVec, dplyr::across(contains("dim_")), .keep_all = TRUE)
print("‚úÖ Multi-word vector averaging complete.")
return(cleanVec)
}
runApp()
get_centroid <- function(dictionary, vectors) {
print("üîπ Step 1: Computing centroid from word embeddings")
cleanVec <- tryCatch({
multi_word_avg(dictionary, vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in `multi_word_avg()`: ", e$message))
})
if (nrow(cleanVec) == 0) {
stop("‚ùå Error: No valid word vectors found for `dictionary`.")
}
print("‚úÖ Extracted multi-word vectors, computing centroid.")
centroid <- colMeans(cleanVec[, -1, drop = FALSE])
centroid <- centroid / norm_L2(centroid)
print("‚úÖ Centroid computation complete.")
return(centroid)
}
runApp()
multi_word_avg <- function(vocabulary, vectors) {
print("üîπ Step 1: Extracting word vectors for multi-word terms")
dictionarySplit <- stringr::str_split(vocabulary, pattern = "[- _]")
cleanWords <- clean_vocabulary(vocabulary)
# Check if embeddings exist
if (!"embeddings" %in% names(vectors)) {
stop("‚ùå Error: `vectors` does not contain an `embeddings` element.")
}
wordVec <- vectors$embeddings %>%
dplyr::filter(word %in% tolower(as.character(unique(unlist(dictionarySplit)))))
# Check if wordVec is empty
if (nrow(wordVec) == 0) {
stop("‚ùå Error: No words from `vocabulary` found in `vectors$embeddings`.")
}
cleanVec <- wordVec[FALSE,]
print(paste0("‚úÖ Extracted ", nrow(wordVec), " words for vector averaging."))
for (i in seq_along(dictionarySplit)) {
wordInd <- tolower(unlist(dictionarySplit[[i]]))
wordFull <- paste(wordInd, collapse = "_")
rawVec <- wordVec %>%
dplyr::filter(word %in% wordInd)
if (nrow(rawVec) > 1) {
tmpVec <- rawVec[, -1] %>%
dplyr::summarise(dplyr::across(.cols = everything(), mean)) %>%
dplyr::mutate(word = wordFull) %>%
dplyr::relocate(word)
} else if (nrow(rawVec) == 1) {
tmpVec <- rawVec[, -1] %>%
dplyr::mutate(word = wordFull) %>%
dplyr::relocate(word)
} else {
next  # Skip if no matching words
}
cleanVec <- dplyr::bind_rows(cleanVec, tmpVec)
}
if (nrow(cleanVec) == 0) {
stop("‚ùå Error: `multi_word_avg` produced an empty result.")
}
cleanVec <- dplyr::distinct(cleanVec, dplyr::across(contains("dim_")), .keep_all = TRUE)
print("‚úÖ Multi-word vector averaging complete.")
return(cleanVec)
}
seedWords
xx <- multi_word_avg(seedWords, glove_data$embeddings)
View(glove_data)
xx <- multi_word_avg(seedWords, glove_data)
View(xx)
xx<-get_keywords(seedWords, vectorsGlove)
xx<-get_keywords(seedWords, glove_data)
xx <- multi_word_avg(seedWords, glove_data)
centroid <- get_centroid(seedWords, glove_data)
xx<-get_keywords(seedWords, glove_data)
# Subset Multi-Word Phrases # This can be done going into the function # saves dictionary
multiWord <- glove_data$metadata$word[stringr::str_detect(glove_data$metadata$word, '[-_]')]
multWordVec <- multi_word_avg(multiWord, glove_data)
multiWord
glove_data$metadata$word
multiWord
get_keywords = function(dictionary, vectors) {
print("üîπ Step 1: Ranking Keywords")
if (!"embeddings" %in% names(vectors) || !"metadata" %in% names(vectors)) {
stop("‚ùå Error: `vectors` does not contain `embeddings` or `metadata`.")
}
# Ensure embeddings and metadata have required columns
if (!"word" %in% colnames(vectors$embeddings)) {
stop("‚ùå Error: `vectors$embeddings` does not have a `word` column.")
}
if (!"word" %in% colnames(vectors$metadata)) {
stop("‚ùå Error: `vectors$metadata` does not have a `word` column.")
}
# Get Centroid
centroid <- tryCatch({
get_centroid(dictionary, vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in `get_centroid()`: ", e$message))
})
print("‚úÖ Centroid calculated successfully.")
# Extract Multi-Word Phrases
multiWord <- vectors$metadata$word[stringr::str_detect(vectors$metadata$word, '[-_]')]
if (length(multiWord) == 0) {
print("‚ö†Ô∏è Warning: No multi-word phrases found. Skipping multi-word vector averaging.")
multWordVec <- tibble::tibble(word = character(), matrix(nrow = 0, ncol = ncol(vectors$embeddings) - 1))
} else {
multWordVec <- tryCatch({
multi_word_avg(multiWord, vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in `multi_word_avg()`: ", e$message))
})
print("‚úÖ Multi-word phrases processed.")
}
# Convert to Matrix
metadata <- vectors$metadata
vectors_matrix <- tryCatch({
vectors_df <- tibble::column_to_rownames(vectors$embeddings, 'word')
if (nrow(multWordVec) > 0) {
vectorsExpand <- tibble::column_to_rownames(multWordVec, 'word')
combined_vectors <- dplyr::bind_rows(vectors_df, vectorsExpand) %>%
distinct(dplyr::across(contains("dim_")), .keep_all = TRUE)
} else {
combined_vectors <- vectors_df
}
data.matrix(combined_vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in matrix conversion: ", e$message))
})
print("‚úÖ Vector matrix created.")
# Compute Similarities
similarities <- tryCatch({
sim_scores <- (vectors_matrix %*% centroid)[, 1]  # Matrix Multiplication
tibble::tibble(word = names(sim_scores), similarity = sim_scores) %>%
dplyr::arrange(-similarity) %>%
dplyr::right_join(metadata, by = 'word')
}, error = function(e) {
stop(paste0("‚ùå Error computing similarities: ", e$message))
})
print("‚úÖ Keyword ranking complete.")
return(similarities)
}
xx<-get_keywords(seedWords, glove_data)
View(xx)
runApp()
get_keywords = function(dictionary, vectors) {
print("üîπ Step 1: Ranking Keywords")
if (!"embeddings" %in% names(vectors) || !"metadata" %in% names(vectors)) {
stop("‚ùå Error: `vectors` does not contain `embeddings` or `metadata`.")
}
# Ensure embeddings and metadata have required columns
if (!"word" %in% colnames(vectors$embeddings)) {
stop("‚ùå Error: `vectors$embeddings` does not have a `word` column.")
}
if (!"word" %in% colnames(vectors$metadata)) {
stop("‚ùå Error: `vectors$metadata` does not have a `word` column.")
}
# Get Centroid
centroid <- tryCatch({
get_centroid(dictionary, vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in `get_centroid()`: ", e$message))
})
print("‚úÖ Centroid calculated successfully.")
# Extract Multi-Word Phrases
multiWord <- vectors$metadata$word[stringr::str_detect(vectors$metadata$word, '[-_]')]
if (length(multiWord) == 0) {
print("‚ö†Ô∏è Warning: No multi-word phrases found. Skipping multi-word vector averaging.")
multWordVec <- tibble::tibble(word = character(), matrix(nrow = 0, ncol = ncol(vectors$embeddings) - 1))
} else {
multWordVec <- tryCatch({
multi_word_avg(multiWord, vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in `multi_word_avg()`: ", e$message))
})
print("‚úÖ Multi-word phrases processed.")
}
# Convert to Matrix
metadata <- vectors$metadata
vectors_matrix <- tryCatch({
vectors_df <- tibble::column_to_rownames(vectors$embeddings, 'word')
if (nrow(multWordVec) > 0) {
vectorsExpand <- tibble::column_to_rownames(multWordVec, 'word')
combined_vectors <- dplyr::bind_rows(vectors_df, vectorsExpand) %>%
distinct(dplyr::across(contains("dim_")), .keep_all = TRUE)
} else {
combined_vectors <- vectors_df
}
data.matrix(combined_vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in matrix conversion: ", e$message))
})
print("‚úÖ Vector matrix created.")
# Compute Similarities
similarities <- tryCatch({
sim_scores <- (vectors_matrix %*% centroid)[, 1]  # Matrix Multiplication
tibble::tibble(word = names(sim_scores), similarity = sim_scores) %>%
dplyr::arrange(-similarity) %>%
dplyr::right_join(metadata, by = 'word')
}, error = function(e) {
stop(paste0("‚ùå Error computing similarities: ", e$message))
})
print("‚úÖ Keyword ranking complete.")
return(similarities)
}
xx<-get_keywords(seedWords, glove_data)
xx
get_keywords = function(dictionary, vectors) {
print("üîπ Step 1: Ranking Keywords")
if (!"embeddings" %in% names(vectors) || !"metadata" %in% names(vectors)) {
stop("‚ùå Error: `vectors` does not contain `embeddings` or `metadata`.")
}
# Ensure embeddings and metadata have required columns
if (!"word" %in% colnames(vectors$embeddings)) {
stop("‚ùå Error: `vectors$embeddings` does not have a `word` column.")
}
if (!"word" %in% colnames(vectors$metadata)) {
stop("‚ùå Error: `vectors$metadata` does not have a `word` column.")
}
# Get Centroid
centroid <- tryCatch({
get_centroid(dictionary, vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in `get_centroid()`: ", e$message))
})
print("‚úÖ Centroid calculated successfully.")
# Extract Multi-Word Phrases
multiWord <- vectors$metadata$word[stringr::str_detect(vectors$metadata$word, '[-_]')]
if (length(multiWord) == 0) {
print("‚ö†Ô∏è Warning: No multi-word phrases found. Skipping multi-word vector averaging.")
multWordVec <- tibble::tibble(word = character(), matrix(nrow = 0, ncol = ncol(vectors$embeddings) - 1))
} else {
multWordVec <- tryCatch({
multi_word_avg(multiWord, vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in `multi_word_avg()`: ", e$message))
})
print("‚úÖ Multi-word phrases processed.")
}
# Convert to Matrix
metadata <- vectors$metadata
vectors_matrix <- tryCatch({
vectors_df <- tibble::column_to_rownames(vectors$embeddings, 'word')
if (nrow(multWordVec) > 0) {
vectorsExpand <- tibble::column_to_rownames(multWordVec, 'word')
combined_vectors <- dplyr::bind_rows(vectors_df, vectorsExpand) %>%
distinct(dplyr::across(contains("dim_")), .keep_all = TRUE)
} else {
combined_vectors <- vectors_df
}
data.matrix(combined_vectors)
}, error = function(e) {
stop(paste0("‚ùå Error in matrix conversion: ", e$message))
})
print("‚úÖ Vector matrix created.")
# Compute Similarities
similarities <- tryCatch({
sim_scores <- (vectors_matrix %*% centroid)[, 1]  # Matrix Multiplication
tibble::tibble(word = names(sim_scores), similarity = sim_scores) %>%
dplyr::arrange(-similarity) %>%
dplyr::right_join(metadata, by = 'word')
}, error = function(e) {
stop(paste0("‚ùå Error computing similarities: ", e$message))
})
print("‚úÖ Keyword ranking complete.")
return(similarities)
}
xx<-get_keywords(seedWords, glove_data)
View(xx)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
install.packages("quanteda.textstats")
library(quanteda)
library(quanteda.textstats)
install.packages(c("quanteda", "quanteda.textstats", "Matrix"))
install.packages(c("quanteda", "quanteda.textstats", "Matrix"))
shiny::runApp()
runApp()
runApp()
ud_model <- udpipe_load_model("english-ewt-ud-2.5-191206.udpipe")  # Load model
shiny::runApp()
View(load_glove)
runApp()
runApp()
runApp()
uk_eng_corp_sample <- readRDS("C:/Users/isaac/OneDrive/Escritorio/rKeywords-master/rKeywords-master/app/uk_eng_corp_sample.rds")
shiny::runApp()
runApp('C:/Users/isaac/OneDrive/Escritorio/rKeywords-master/rKeywords-master')
