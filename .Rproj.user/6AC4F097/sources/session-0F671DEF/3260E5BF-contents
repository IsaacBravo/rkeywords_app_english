library(udpipe)
library(quanteda)
library(dplyr)
library(stringr)
library(udpipe)
library(dplyr)







rawString <- "Immigrant* OR migrant* OR asylum seeker* OR visa*"
seedWords <- convert_bool(rawString)
rawCorp <- readRDS("C:/Users/isaac/OneDrive/Escritorio/rKeywords-master/rKeywords-master/data/uk_eng_corp_sample.rds")

tidyCorp <- clean_corpus_emmy(seedWords, rawCorp, minSentenceChar = 3)
tidyCorp2 <- clean_corpus(seedWords, rawCorp, minSentenceChar = 3)

tidyCorp <- tag_corpus_emmy(tidyCorp)
tidyCorp2 <- tag_corpus(tidyCorp2)


modelPath <- 'C:/Users/isaac/OneDrive/Escritorio/rKeywords-master/rKeywords-master/app/models/glove.6B.50d.txt'

# spacy_install()
# spacy_download_langmodel("en_core_web_lg")

spacy_initialize()
spacy_initialize(save_profile = TRUE) # Optional

spacy_initialize(condaenv = "spacy_env", model = "en_core_web_sm",
                 save_profile = TRUE)

tidyCorp <- clean_corpus(seedWords, rawCorp, minSentenceChar = 3)
tidyCorp <- tag_corpus(tidyCorp)

keepPOS = c("NOUN", "ADJ", "X", "PROPN", "ENTITY", "nounphrase", "PRON")

filter_corpus <- function(tagCorp, keepList) {

  #' Filter parts of speech, punctuation, and stopwords from corpus.
  #' @param tagCorp cleaned and tagged corpus
  #' @param keepList list of POS types to keep
  #' @return tokenized and cleaned corpus with POS choices.
  #'
  # Load Required Packages
  require(quanteda.textstats, quietly = TRUE)

  # Replace Token with Lemma
  parsedTextCl <- tagCorp %>%
    dplyr::select(-token) %>%
    dplyr::rename("token" = lemma) %>%
    dplyr::filter(upos %in% keepList)

  # Ensure token list is structured properly
  token_list <- split(parsedTextCl$token, parsedTextCl$doc_id)

  # Convert to quanteda tokens
  clText <- quanteda::tokens(token_list,
                             remove_punct = TRUE,
                             remove_symbols = TRUE,
                             remove_numbers = TRUE,
                             remove_url = TRUE) %>%
    quanteda::tokens_remove(quanteda::stopwords("en")) %>%
    quanteda::tokens_tolower()

  # Get unique words
  allWords <- unique(unlist(clText))


  return(allWords)

}

x <- filter_corpus(tidyCorp, keepPOS)

tagCorpCl <- tidyCorp %>%
  filter(tolower(lemma) %in% x)

get_vocabulary <- function(parsedText, minTermFrequency = 2) {

  #' Get filtered list of unique corpus domain relevant words.
  #' @param parsedText fully cleaned corpus with tagging and filtering.
  #' @param minTermFrequency minimum number of times a word appears in a text to be kept.
  #' @return unique list of words in corpus.

  print("üîπ Step 1: Checking and cleaning entity_type column")

  # Ensure entity_type column exists, otherwise create it
  if (!"entity_type" %in% colnames(parsedText)) {
    parsedText$entity_type <- "X"
  } else {
    parsedText$entity_type <- ifelse(is.na(parsedText$entity_type) | parsedText$entity_type == "", "X", parsedText$entity_type)
  }

  print("‚úÖ entity_type column cleaned")

  print("üîπ Step 2: Creating word column from lemma")
  parsedText$word <- stringr::str_trim(tolower(parsedText$lemma), side = "both")

  print("‚úÖ Word column created")

  ### Parts of Speech Processing
  print("üîπ Step 3: Processing POS tagging")

  parsedTmp <- parsedText %>%
    dplyr::group_by(word, entity_type) %>%
    dplyr::summarise(Count = dplyr::n(), pos = first(upos), .groups = "drop") %>%
    dplyr::distinct(word, entity_type, Count, pos, .keep_all = TRUE)

  print("‚úÖ POS tagging done")

  print("üîπ Step 4: Computing percentages and filtering words")

  parsedTmp <- parsedTmp %>%
    dplyr::group_by(word) %>%
    dplyr::mutate(Percentage = round(Count / sum(Count) * 100, 2)) %>%
    dplyr::slice_max(order_by = Percentage, n = 1) %>%
    dplyr::distinct(word, .keep_all = TRUE)

  print("‚úÖ Percentage calculations complete")

  ### Summarizing Counts
  print("üîπ Step 5: Summarizing word occurrences")

  parsedTmp <- parsedTmp %>%
    dplyr::group_by(word, entity_type) %>%
    dplyr::summarise(TotCount = sum(Count), Percentage = first(Percentage), pos = first(pos), .groups = "drop")

  print("‚úÖ Word summarization done")

  print("üîπ Step 6: Filtering words based on minTermFrequency")

  parsedTextFin <- parsedTmp %>%
    dplyr::filter(TotCount >= minTermFrequency) %>%
    dplyr::ungroup()

  print("‚úÖ Filtering complete")

  return(parsedTextFin)
}


get_vocabulary(tagCorpCl)


load_glove = function(filename, corpus, n=5000, minTermFrequency = 2) {

  #' Filter domain relevant vocabulary from pre-trained GloVe embeddings.
  #' @param filename path to pre-trained GloVe word embedding text file.
  #' @param corpus cleaned text corpus
  #' @param n total top words to keep from cleaned text corpus
  #' @param minTermFrequency minimum occurrences of a word in training text
  #' @return list of embeddings relevant to domain vocabulary.

  print("üîπ Step 1: Starting load_glove() function")

  # Load Required Packages
  require(vroom, quietly = TRUE)

  print("üîπ Step 2: Checking if file exists")

  filename = path.expand(filename)
  if (!file.exists(filename)) {
    stop(paste0("‚ùå Error: File ", filename, " does not exist"))
  }

  print("‚úÖ File found: Proceeding with loading vocabulary")

  print("üîπ Step 3: Extracting vocabulary from corpus")

  vocabulary <- tryCatch({
    get_vocabulary(corpus, minTermFrequency = minTermFrequency)  %>%
      dplyr::arrange(desc(TotCount)) %>%
      dplyr::slice_head(n=n) %>%
      dplyr::select(word, "total_count" = TotCount, "part_of_speech" = pos, entity_type)
  }, error = function(e) {
    stop(paste0("‚ùå Error in get_vocabulary(): ", e$message))
  })

  print(paste0("‚úÖ Extracted vocabulary: ", nrow(vocabulary), " words found"))

  print("üîπ Step 4: Cleaning vocabulary")
  vocabSplit <- tryCatch({
    clean_vocabulary(vocabulary$word)
  }, error = function(e) {
    stop(paste0("‚ùå Error in clean_vocabulary(): ", e$message))
  })

  print(paste0("‚úÖ Vocabulary cleaned: ", length(vocabSplit), " words retained"))
  gc()

  print("üîπ Step 5: Loading GloVe model")
  gl_model <- tryCatch({
    vroom::vroom(filename, delim = " ", vroom::locale(encoding = "UTF-8"), quote = "Z",
                 skip_empty_rows = TRUE, skip = 1) %>%
      dplyr::filter(X1 %in% vocabSplit)
  }, error = function(e) {
    stop(paste0("‚ùå Error loading GloVe embeddings: ", e$message))
  })

  print(paste0("‚úÖ Loaded GloVe model with ", nrow(gl_model), " words matching vocabulary"))

  gc()

  print("üîπ Step 6: Normalizing GloVe vectors")
  if (ncol(gl_model) > 1) {
    gl_model <- tryCatch({
      gl_model %>%
        dplyr::mutate(across(2:ncol(gl_model), ~ scale(., center = FALSE, scale = norm_L2(.))))
    }, error = function(e) {
      stop(paste0("‚ùå Error in normalization step: ", e$message))
    })
    print("‚úÖ Normalization complete")
  } else {
    stop("‚ùå Error: GloVe model has only one column (word names) and no embeddings")
  }

  print("üîπ Step 7: Renaming columns")
  colnames(gl_model) = c('word', paste('dim', 1:(ncol(gl_model)-1), sep = '_'))

  print("‚úÖ Column renaming done")

  model_results <- list("metadata" = vocabulary, "embeddings" = gl_model)

  print("‚úÖ Function successfully completed! Returning results.")

  return(model_results)
}

glove_data <- load_glove(modelPath, tagCorpCl, n=5000, minTermFrequency=2)



multi_word_avg <- function(vocabulary, vectors) {

  print("üîπ Step 1: Extracting word vectors for multi-word terms")

  dictionarySplit <- stringr::str_split(vocabulary, pattern = "[- _]")
  cleanWords <- clean_vocabulary(vocabulary)

  # Check if embeddings exist
  if (!"embeddings" %in% names(vectors)) {
    stop("‚ùå Error: `vectors` does not contain an `embeddings` element.")
  }

  wordVec <- vectors$embeddings %>%
    dplyr::filter(word %in% tolower(as.character(unique(unlist(dictionarySplit)))))

  # Check if wordVec is empty
  if (nrow(wordVec) == 0) {
    stop("‚ùå Error: No words from `vocabulary` found in `vectors$embeddings`.")
  }

  cleanVec <- wordVec[FALSE,]

  print(paste0("‚úÖ Extracted ", nrow(wordVec), " words for vector averaging."))

  for (i in seq_along(dictionarySplit)) {

    wordInd <- tolower(unlist(dictionarySplit[[i]]))
    wordFull <- paste(wordInd, collapse = "_")

    rawVec <- wordVec %>%
      dplyr::filter(word %in% wordInd)

    if (nrow(rawVec) > 1) {
      tmpVec <- rawVec[, -1] %>%
        dplyr::summarise(dplyr::across(.cols = everything(), mean)) %>%
        dplyr::mutate(word = wordFull) %>%
        dplyr::relocate(word)
    } else if (nrow(rawVec) == 1) {
      tmpVec <- rawVec[, -1] %>%
        dplyr::mutate(word = wordFull) %>%
        dplyr::relocate(word)
    } else {
      next  # Skip if no matching words
    }

    cleanVec <- dplyr::bind_rows(cleanVec, tmpVec)
  }

  if (nrow(cleanVec) == 0) {
    stop("‚ùå Error: `multi_word_avg` produced an empty result.")
  }

  cleanVec <- dplyr::distinct(cleanVec, dplyr::across(contains("dim_")), .keep_all = TRUE)

  print("‚úÖ Multi-word vector averaging complete.")

  return(cleanVec)
}

get_centroid <- function(dictionary, vectors) {

  print("üîπ Step 1: Computing centroid from word embeddings")

  cleanVec <- tryCatch({
    multi_word_avg(dictionary, vectors)
  }, error = function(e) {
    stop(paste0("‚ùå Error in `multi_word_avg()`: ", e$message))
  })

  if (nrow(cleanVec) == 0) {
    stop("‚ùå Error: No valid word vectors found for `dictionary`.")
  }

  print("‚úÖ Extracted multi-word vectors, computing centroid.")

  centroid <- colMeans(cleanVec[, -1, drop = FALSE])
  centroid <- centroid / norm_L2(centroid)

  print("‚úÖ Centroid computation complete.")

  return(centroid)
}

centroid <- get_centroid(seedWords, glove_data)

# Subset Multi-Word Phrases # This can be done going into the function # saves dictionary
multiWord <- glove_data$metadata$word[stringr::str_detect(glove_data$metadata$word, '[-_]')]

multWordVec <- multi_word_avg(multiWord, glove_data)




xx <- multi_word_avg(seedWords, glove_data)

xx<-get_keywords(seedWords, glove_data)


get_keywords = function(dictionary, vectors) {

  print("üîπ Step 1: Ranking Keywords")

  if (!"embeddings" %in% names(vectors) || !"metadata" %in% names(vectors)) {
    stop("‚ùå Error: `vectors` does not contain `embeddings` or `metadata`.")
  }

  # Ensure embeddings and metadata have required columns
  if (!"word" %in% colnames(vectors$embeddings)) {
    stop("‚ùå Error: `vectors$embeddings` does not have a `word` column.")
  }

  if (!"word" %in% colnames(vectors$metadata)) {
    stop("‚ùå Error: `vectors$metadata` does not have a `word` column.")
  }

  # Get Centroid
  centroid <- tryCatch({
    get_centroid(dictionary, vectors)
  }, error = function(e) {
    stop(paste0("‚ùå Error in `get_centroid()`: ", e$message))
  })

  print("‚úÖ Centroid calculated successfully.")

  # Extract Multi-Word Phrases
  multiWord <- vectors$metadata$word[stringr::str_detect(vectors$metadata$word, '[-_]')]

  if (length(multiWord) == 0) {
    print("‚ö†Ô∏è Warning: No multi-word phrases found. Skipping multi-word vector averaging.")
    multWordVec <- tibble::tibble(word = character(), matrix(nrow = 0, ncol = ncol(vectors$embeddings) - 1))
  } else {
    multWordVec <- tryCatch({
      multi_word_avg(multiWord, vectors)
    }, error = function(e) {
      stop(paste0("‚ùå Error in `multi_word_avg()`: ", e$message))
    })
    print("‚úÖ Multi-word phrases processed.")
  }

  # Convert to Matrix
  metadata <- vectors$metadata

  vectors_matrix <- tryCatch({
    vectors_df <- tibble::column_to_rownames(vectors$embeddings, 'word')

    if (nrow(multWordVec) > 0) {
      vectorsExpand <- tibble::column_to_rownames(multWordVec, 'word')
      combined_vectors <- dplyr::bind_rows(vectors_df, vectorsExpand) %>%
        distinct(dplyr::across(contains("dim_")), .keep_all = TRUE)
    } else {
      combined_vectors <- vectors_df
    }

    data.matrix(combined_vectors)
  }, error = function(e) {
    stop(paste0("‚ùå Error in matrix conversion: ", e$message))
  })

  print("‚úÖ Vector matrix created.")

  # Compute Similarities
  similarities <- tryCatch({
    sim_scores <- (vectors_matrix %*% centroid)[, 1]  # Matrix Multiplication
    tibble::tibble(word = names(sim_scores), similarity = sim_scores) %>%
      dplyr::arrange(-similarity) %>%
      dplyr::right_join(metadata, by = 'word')
  }, error = function(e) {
    stop(paste0("‚ùå Error computing similarities: ", e$message))
  })

  print("‚úÖ Keyword ranking complete.")

  return(similarities)
}














keywordsNew <- automate_keywords(seedWords = seedWords,
                                 corpus = rawCorp,
                                 modelPath = modelPath,
                                 nCandidates = 10)

queryNew <- create_query(keywordsNew, n = 40, type = "regex")
print(queryNew)



